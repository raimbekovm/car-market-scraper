"""
–ü–ê–†–°–ï–† DROM.RU
–°–∫—Ä–∞–ø–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —Å —Å–∞–π—Ç–∞ auto.drom.ru
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import json
import time
from typing import List, Dict, Any, Set, Optional
import random
import os
import re
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ brotli
try:
    import brotli
    print("‚úÖ Brotli —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
except ImportError:
    print("‚ùå –û–®–ò–ë–ö–ê: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ brotli –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
    print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë –∫–æ–º–∞–Ω–¥–æ–π: pip install brotli")
    exit(1)

# –ü–æ–ª—É—á–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Å–∫—Ä–∏–ø—Ç
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# –ù–ê–°–¢–†–û–ô–ö–ê: –ø–æ—Ä–æ–≥ –¥–ª—è "–º–∞–ª–æ–≥–æ" –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
SMALL_BRAND_THRESHOLD = 20


# ========== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–ë–û–¢–´ –° –ü–†–û–ì–†–ï–°–°–û–ú ==========

def load_progress():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–∑ —Ñ–∞–π–ª–∞"""
    progress_file = os.path.join(SCRIPT_DIR, 'drom_scraped_data_progress.json')
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()

                if not content:
                    print(f"‚ö†Ô∏è  –§–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø—É—Å—Ç–æ–π, –Ω–∞—á–∏–Ω–∞–µ–º —Å–Ω–∞—á–∞–ª–∞")
                    return None

                data = json.loads(content)

                if 'last_index' not in data or 'statistics' not in data:
                    print(f"‚ö†Ô∏è  –§–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω")
                    return None

                print(f"\nüìÇ –ù–ê–ô–î–ï–ù –§–ê–ô–õ –ü–†–û–ì–†–ï–°–°–ê")
                print(f"   –ü–æ—Å–ª–µ–¥–Ω—è—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞: {data.get('last_index', -1) + 1}")
                print(f"   –£—Å–ø–µ—à–Ω–æ: {data['statistics'].get('successful', 0)}")
                print(f"   –û—à–∏–±–æ–∫: {data['statistics'].get('failed', 0)}")
                print(f"   –ë–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {data['statistics'].get('no_results', 0)}")
                return data
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
            return None
    return None


# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
excel_file = os.path.join(SCRIPT_DIR, '–Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø–æ–∫–æ–ª–µ–Ω–∏—è_updated2.xlsx')
df = pd.read_excel(excel_file)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
progress_data = load_progress()

if progress_data:
    start_index = progress_data.get('last_index', -1) + 1
    results = progress_data.get('results', [])
    statistics = progress_data.get('statistics', {
        'total_rows': len(df),
        'successful': 0,
        'failed': 0,
        'no_results': 0,
        'total_ads_found': 0,
        'skipped_brands': 0,
        'errors': []
    })
    brand_cache = progress_data.get('brand_cache', {})
    request_count = progress_data.get('request_count', 0)

    print(f"üîÑ –ü–†–û–î–û–õ–ñ–ê–ï–ú –° –°–¢–†–û–ö–ò {start_index + 1} –∏–∑ {len(df)}")
    print(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {start_index}/{len(df)} ({start_index / len(df) * 100:.1f}%)")
    print(f"   –í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {request_count}")
    print(f"{'=' * 70}\n")
else:
    start_index = 0
    results = []
    statistics = {
        'total_rows': len(df),
        'successful': 0,
        'failed': 0,
        'no_results': 0,
        'total_ads_found': 0,
        'skipped_brands': 0,
        'errors': []
    }
    brand_cache = {}
    request_count = 0

    print(f"üÜï –ù–ê–ß–ò–ù–ê–ï–ú –° –ù–ê–ß–ê–õ–ê")
    print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df)}")
    print(f"{'=' * 70}\n")

# User-Agent —Å–ø–∏—Å–æ–∫
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
]


def create_session():
    """–°–æ–∑–¥–∞–µ—Ç —Å–µ—Å—Å–∏—é —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    session = requests.Session()
    retry_strategy = Retry(
        total=5,
        backoff_factor=2,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session


def get_headers():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }


def decode_response(response):
    """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —É—á–µ—Ç–æ–º Content-Encoding –∏ charset"""
    content_encoding = response.headers.get('Content-Encoding', '').lower()
    content_type = response.headers.get('Content-Type', '')

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –∏–∑ Content-Type (–æ–±—ã—á–Ω–æ windows-1251 –¥–ª—è drom.ru)
    encoding = 'windows-1251'
    if 'charset=' in content_type:
        encoding = content_type.split('charset=')[-1].strip()

    # –ß–∏—Ç–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
    raw_content = response.raw.read()

    try:
        if content_encoding == 'br':
            return brotli.decompress(raw_content).decode(encoding)
        elif content_encoding == 'gzip':
            import gzip
            return gzip.decompress(raw_content).decode(encoding)
        else:
            return raw_content.decode(encoding)
    except UnicodeDecodeError:
        # Fallback –Ω–∞ windows-1251
        if content_encoding == 'br':
            return brotli.decompress(raw_content).decode('windows-1251')
        elif content_encoding == 'gzip':
            import gzip
            return gzip.decompress(raw_content).decode('windows-1251')
        else:
            return raw_content.decode('windows-1251')


def adaptive_delay(request_count, base_min=0.5, base_max=1):
    """–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–∞—É–∑–∞ - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è drom.ru"""
    multiplier = 1 + (request_count // 50) * 0.1
    min_delay = min(base_min * multiplier, 3)
    max_delay = min(base_max * multiplier, 5)
    return random.uniform(min_delay, max_delay)


def long_break_check(request_count, break_every=200, break_duration=30):
    """–î–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ –∫–∞–∂–¥—ã–µ N –∑–∞–ø—Ä–æ—Å–æ–≤ - —Ä–µ–¥–∫–æ –∏ –∫–æ—Ä–æ—Ç–∫–æ"""
    if request_count > 0 and request_count % break_every == 0:
        print(f"\n    ‚è∏Ô∏è  –î–õ–ò–ù–ù–ê–Ø –ü–ê–£–ó–ê –ø–æ—Å–ª–µ {request_count} –∑–∞–ø—Ä–æ—Å–æ–≤")
        print(f"    ‚è∏Ô∏è  –ñ–¥—ë–º {break_duration} —Å–µ–∫—É–Ω–¥...")
        time.sleep(break_duration)
        return True
    return False


def save_progress(current_index):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å"""
    progress_file = os.path.join(SCRIPT_DIR, 'drom_scraped_data_progress.json')

    # –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è
    if os.path.exists(progress_file):
        backup_file = os.path.join(SCRIPT_DIR, 'drom_scraped_data_progress.json.backup')
        try:
            import shutil
            shutil.copy2(progress_file, backup_file)
        except:
            pass

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
    progress_data = {
        'last_index': current_index,
        'statistics': statistics,
        'results': results,
        'brand_cache': brand_cache,
        'request_count': request_count
    }

    try:
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"    ‚ùå –û–®–ò–ë–ö–ê —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
        return

    if results:
        excel_data = []
        for result in results:
            if result['total_ads'] == 0:
                excel_data.append({
                    'brand': result['brand'],
                    'model': result['model'],
                    'start_year': result['start_year'],
                    'finish_year': result['finish_year'],
                    'search_url': result['search_url'],
                    'status': result['error'] or '–ù–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π',
                    'car_name': '',
                    'year': '',
                    'price': '',
                    'currency': '',
                    'url': '',
                    'mileage': '',
                    'vin': '',
                    'image_url': ''
                })
            else:
                for listing in result['listings']:
                    excel_data.append({
                        'brand': result['brand'],
                        'model': result['model'],
                        'start_year': result['start_year'],
                        'finish_year': result['finish_year'],
                        'search_url': result['search_url'],
                        'status': '–ù–∞–π–¥–µ–Ω–æ',
                        'car_name': listing.get('name', ''),
                        'year': listing.get('year', ''),
                        'price': listing.get('price', ''),
                        'currency': listing.get('currency', 'RUB'),
                        'url': listing.get('url', ''),
                        'mileage': listing.get('mileage', ''),
                        'vin': listing.get('vin', ''),
                        'image_url': listing.get('image', '')
                    })

        results_file = os.path.join(SCRIPT_DIR, 'drom_scraped_data_progress.xlsx')
        try:
            pd.DataFrame(excel_data).to_excel(results_file, index=False)
        except Exception as e:
            print(f"    ‚ùå –û–®–ò–ë–ö–ê —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Excel: {e}")
            return

    print("    üíæ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω")


def parse_json_ld_listings(html: str) -> List[Dict[str, Any]]:
    """–ü–∞—Ä—Å–∏—Ç JSON-LD –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ HTML"""
    soup = BeautifulSoup(html, 'html.parser')
    listings = []

    json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})

    for script in json_ld_scripts:
        if not script.string:
            continue

        try:
            data = json.loads(script.string)

            if isinstance(data, dict) and data.get('@type') == 'Car':
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è
                listing = {
                    'name': data.get('name', ''),
                    'brand': data.get('brand', {}).get('name', ''),
                    'model': data.get('model', ''),
                    'year': data.get('vehicleModelDate', ''),
                    'price': data.get('offers', {}).get('price'),
                    'currency': data.get('offers', {}).get('priceCurrency', 'RUB'),
                    'url': data.get('offers', {}).get('url', ''),
                    'image': data.get('image', {}).get('url', ''),
                    'mileage': data.get('mileageFromOdometer', {}).get('value'),
                    'vin': data.get('vehicleIdentificationNumber', ''),
                }
                listings.append(listing)
        except:
            pass

    return listings


def fetch_brand_listings(brand: str, session, max_pages: int = 3) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è –±—Ä–µ–Ω–¥–∞"""
    global request_count

    url = f"https://auto.drom.ru/{brand}/"
    all_listings = []
    seen_urls = set()

    print(f"    üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –±—Ä–µ–Ω–¥–∞: {url}")

    for page in range(1, max_pages + 1):
        try:
            if page > 1:
                current_url = f"{url}page{page}/"
            else:
                current_url = url

            response = session.get(current_url, headers=get_headers(), timeout=45, stream=True)
            request_count += 1

            if response.status_code == 429:
                print(f"    ‚ö†Ô∏è –ö–æ–¥ 429, –ø–∞—É–∑–∞ 5 –º–∏–Ω—É—Ç...")
                time.sleep(300)
                continue

            if response.status_code != 200:
                print(f"    ‚ö† –°—Ç–∞—Ç—É—Å {response.status_code}")
                break

            html = decode_response(response)
            page_listings = parse_json_ld_listings(html)

            if not page_listings:
                if page == 1:
                    print(f"    ‚ö† –û–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                break

            # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ URL
            new_count = 0
            for listing in page_listings:
                url_key = listing.get('url', '')
                if url_key and url_key not in seen_urls:
                    seen_urls.add(url_key)
                    all_listings.append(listing)
                    new_count += 1

            print(f"    –°—Ç—Ä.{page}: –Ω–∞–π–¥–µ–Ω–æ {len(page_listings)} (–Ω–æ–≤—ã—Ö: {new_count})")

            if page < max_pages and new_count > 0:
                time.sleep(random.uniform(0.3, 0.5))

        except Exception as e:
            print(f"    ‚ö† –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")
            break

    return {
        'count': len(all_listings),
        'listings': all_listings
    }


def filter_listings_by_model(listings: List[Dict], model: str, start_year: Any = None, finish_year: Any = None) -> List[Dict]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ –º–æ–¥–µ–ª–∏ –∏ –≥–æ–¥–∞–º"""
    model_variants = [
        model.lower(),
        model.replace('-', ' ').lower(),
        model.replace('-', '').lower(),
        model.replace(' ', '-').lower(),
    ]

    filtered = []
    for listing in listings:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏
        name = listing.get('name', '').lower()
        listing_model = listing.get('model', '').lower()

        model_match = any(variant in name or variant in listing_model for variant in model_variants)

        if not model_match:
            continue

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ–¥–æ–≤
        if pd.notna(start_year) and pd.notna(finish_year):
            listing_year = listing.get('year', '')
            if listing_year:
                try:
                    year = int(listing_year)
                    if not (int(start_year) <= year <= int(finish_year)):
                        continue
                except:
                    pass

        filtered.append(listing)

    return filtered


def scrape_brand_model(brand: str, model: str, start_year: Any, finish_year: Any, session) -> Dict[str, Any]:
    """–°–∫—Ä–∞–ø–∏—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    global request_count

    base_url = f"https://auto.drom.ru/{brand}/{model}/"

    if pd.notna(start_year) and pd.notna(finish_year):
        search_url = f"{base_url}?minyear={int(start_year)}&maxyear={int(finish_year)}"
    else:
        search_url = base_url

    result = {
        'brand': brand,
        'model': model,
        'start_year': start_year,
        'finish_year': finish_year,
        'search_url': search_url,
        'total_ads': 0,
        'listings': [],
        'error': None
    }

    page = 1
    consecutive_failures = 0
    seen_urls: Set[str] = set()

    print(f"    üîó {search_url}")

    while True:  # –ü–∞—Ä—Å–∏–º –ø–æ–∫–∞ –µ—Å—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        try:
            if page > 1:
                if pd.notna(start_year) and pd.notna(finish_year):
                    current_url = f"{base_url}page{page}/?minyear={int(start_year)}&maxyear={int(finish_year)}"
                else:
                    current_url = f"{base_url}page{page}/"
            else:
                current_url = search_url

            response = session.get(current_url, headers=get_headers(), timeout=45, stream=True)
            request_count += 1

            if response.status_code == 429:
                print(f"    ‚ö†Ô∏è –ö–æ–¥ 429")
                time.sleep(300)
                consecutive_failures += 1
                continue

            if response.status_code != 200:
                result['error'] = f"HTTP {response.status_code}"
                break

            html = decode_response(response)
            page_listings = parse_json_ld_listings(html)

            if not page_listings:
                print(f"    –°—Ç—Ä.{page}: –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (–∫–æ–Ω–µ—Ü)")
                break

            new_listings = []
            for listing in page_listings:
                url_key = listing.get('url', '')
                if url_key and url_key not in seen_urls:
                    seen_urls.add(url_key)
                    new_listings.append(listing)

            if not new_listings:
                print(f"    –°—Ç—Ä.{page}: –≤—Å–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (–∫–æ–Ω–µ—Ü)")
                break

            result['listings'].extend(new_listings)
            result['total_ads'] += len(new_listings)

            duplicates = len(page_listings) - len(new_listings)
            if duplicates > 0:
                print(f"    –°—Ç—Ä.{page}: {len(new_listings)} –Ω–æ–≤—ã—Ö + {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
            else:
                print(f"    –°—Ç—Ä.{page}: {len(new_listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –µ—Å–ª–∏ –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –º–µ–Ω—å—à–µ 20 –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è
            if page == 1 and len(new_listings) < 20:
                print(f"    üí° –ù–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –º–µ–Ω—å—à–µ 20 –æ–±—ä—è–≤–ª–µ–Ω–∏–π - —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
                break

            consecutive_failures = 0
            page += 1

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
            page_delay = random.uniform(0.3, 0.5)
            time.sleep(page_delay)

        except Exception as e:
            consecutive_failures += 1
            print(f"    ‚ö† –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä.{page}: {e}")
            if consecutive_failures >= 5:
                result['error'] = str(e)
                break
            time.sleep(random.uniform(10, 20))

    return result


# ========== –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ ==========

session = create_session()

try:
    current_brand = None
    brand_models_list = []
    skip_to_index = None
    idx = start_index

    while idx < len(df):
        # –ü—Ä–æ–ø—É—Å–∫ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–∞–ª–æ–≥–æ –±—Ä–µ–Ω–¥–∞
        if skip_to_index is not None:
            idx = skip_to_index
            skip_to_index = None
            current_brand = None
            continue

        row = df.iloc[idx]

        brand = row['brand']
        model = row['model']
        start_year = row.get('start_year', None)
        finish_year = row.get('finish_year', None)

        long_break_check(request_count, break_every=200, break_duration=30)

        # –°–º–µ–Ω–∞ –±—Ä–µ–Ω–¥–∞
        if brand != current_brand:
            current_brand = brand

            brand_models_list = df[df['brand'] == brand].to_dict('records')

            print(f"\n{'=' * 70}")
            print(f"üîç –ù–û–í–´–ô –ë–†–ï–ù–î: {brand.upper()}")
            print(f"   –ú–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(brand_models_list)}")
            print('=' * 70)

            if brand not in brand_cache:
                brand_data = fetch_brand_listings(brand, session, max_pages=3)
                brand_cache[brand] = brand_data

                print(f"    üìä –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –±—Ä–µ–Ω–¥–∞: {brand_data['count']}")
                time.sleep(random.uniform(0.5, 1))
            else:
                brand_data = brand_cache[brand]
                print(f"    üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à: {brand_data['count']} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

            # –ë—Ä–µ–Ω–¥ –±–µ–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            if brand_data['count'] == 0:
                print(f"    ‚äó –£ –±—Ä–µ–Ω–¥–∞ {brand.upper()} –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                print(f"    ‚äó –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å–µ {len(brand_models_list)} –º–æ–¥–µ–ª–µ–π...")

                for model_row in brand_models_list:
                    results.append({
                        'brand': brand,
                        'model': model_row['model'],
                        'start_year': model_row.get('start_year'),
                        'finish_year': model_row.get('finish_year'),
                        'search_url': f"https://auto.drom.ru/{brand}/{model_row['model']}/",
                        'total_ads': 0,
                        'listings': [],
                        'error': f"–ë—Ä–µ–Ω–¥ {brand} - –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π"
                    })
                    statistics['no_results'] += 1

                statistics['skipped_brands'] += 1
                last_brand_idx = idx + len(brand_models_list) - 1
                save_progress(last_brand_idx)

                skip_to_index = last_brand_idx + 1
                continue

            # –ú–∞–ª—ã–π –±—Ä–µ–Ω–¥
            elif brand_data['count'] <= SMALL_BRAND_THRESHOLD:
                print(f"    üí° –ú–ê–õ–´–ô –ë–†–ï–ù–î ({brand_data['count']} –æ–±—ä—è–≤–ª–µ–Ω–∏–π)")
                print(f"    üí° –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∏–∑ –æ–±—â–µ–≥–æ —Å–ø–∏—Å–∫–∞")

                for model_row in brand_models_list:
                    model_name = model_row['model']
                    model_start = model_row.get('start_year')
                    model_finish = model_row.get('finish_year')

                    print(f"\n    ‚ûú –ú–æ–¥–µ–ª—å: {model_name}")

                    filtered = filter_listings_by_model(
                        brand_data['listings'],
                        model_name,
                        model_start,
                        model_finish
                    )

                    if filtered:
                        print(f"      ‚úì –ù–∞–π–¥–µ–Ω–æ: {len(filtered)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                        results.append({
                            'brand': brand,
                            'model': model_name,
                            'start_year': model_start,
                            'finish_year': model_finish,
                            'search_url': f"https://auto.drom.ru/{brand}/{model_name}/",
                            'total_ads': len(filtered),
                            'listings': filtered,
                            'error': None
                        })
                        statistics['successful'] += 1
                        statistics['total_ads_found'] += len(filtered)
                    else:
                        print(f"      ‚óã –ù–µ –Ω–∞–π–¥–µ–Ω–æ")
                        results.append({
                            'brand': brand,
                            'model': model_name,
                            'start_year': model_start,
                            'finish_year': model_finish,
                            'search_url': f"https://auto.drom.ru/{brand}/{model_name}/",
                            'total_ads': 0,
                            'listings': [],
                            'error': None
                        })
                        statistics['no_results'] += 1

                last_brand_idx = idx + len(brand_models_list) - 1
                save_progress(last_brand_idx)

                print(f"\n    ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(brand_models_list)} –º–æ–¥–µ–ª–µ–π –±—Ä–µ–Ω–¥–∞ {brand.upper()}")

                skip_to_index = last_brand_idx + 1

                if skip_to_index < len(df):
                    delay = adaptive_delay(request_count, base_min=0.5, base_max=1)
                    print(f"    ‚è≥ –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –Ω–æ–≤—ã–º –±—Ä–µ–Ω–¥–æ–º {delay:.1f}—Å...")
                    time.sleep(delay)

                continue

            else:
                print(f"    ‚úì –ë–û–õ–¨–®–û–ô –ë–†–ï–ù–î ({brand_data['count']} –æ–±—ä—è–≤–ª–µ–Ω–∏–π)")
                print(f"    ‚úì –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é")

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        print(f"\n{'=' * 70}")
        print(f"[{idx + 1}/{len(df)}] {brand.upper()} {model.upper()} ({start_year}-{finish_year})")
        print(f"üìä –í—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {request_count}")
        print('=' * 70)

        result = scrape_brand_model(brand, model, start_year, finish_year, session)
        results.append(result)

        if result['error']:
            statistics['failed'] += 1
            statistics['errors'].append({
                'brand': brand,
                'model': model,
                'url': result['search_url'],
                'error': result['error']
            })
            print(f"    ‚úó –û–®–ò–ë–ö–ê: {result['error']}")
        elif result['total_ads'] == 0:
            statistics['no_results'] += 1
            print(f"    ‚óã –û–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        else:
            statistics['successful'] += 1
            statistics['total_ads_found'] += result['total_ads']
            print(f"    ‚úì –£–°–ü–ï–®–ù–û: {result['total_ads']} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

        save_progress(idx)

        if idx < len(df) - 1:
            delay = adaptive_delay(request_count, base_min=0.5, base_max=1)
            print(f"    ‚è≥ –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–∞—É–∑–∞ {delay:.1f}—Å... (–∑–∞–ø—Ä–æ—Å–æ–≤: {request_count})")
            time.sleep(delay)

        idx += 1

except KeyboardInterrupt:
    print("\n\n‚ö† –ü–†–ï–†–í–ê–ù–û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ú")
    if 'idx' in locals():
        save_progress(idx)
except Exception as e:
    print(f"\n\n‚ö† –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
    import traceback
    traceback.print_exc()
    if 'idx' in locals():
        save_progress(idx)
    raise
finally:
    session.close()

if 'idx' in locals():
    save_progress(idx)
else:
    save_progress(len(df) - 1)

print(f"\n{'=' * 70}")
print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
print('=' * 70)
print(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫:    {statistics['total_rows']}")
print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤:            {request_count}")
print(f"‚úì –£—Å–ø–µ—à–Ω–æ –Ω–∞–π–¥–µ–Ω—ã:         {statistics['successful']}")
print(f"  –í—Å–µ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π:        {statistics['total_ads_found']}")
print(f"‚óã –ë–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:         {statistics['no_results']}")
print(f"‚äó –ü—Ä–æ–ø—É—â–µ–Ω–æ –±—Ä–µ–Ω–¥–æ–≤:       {statistics['skipped_brands']}")
print(f"‚úó –û—à–∏–±–∫–∏:                  {statistics['failed']}")

if statistics['errors']:
    print(f"\n{'-' * 70}")
    print("–°–ü–ò–°–û–ö –û–®–ò–ë–û–ö:")
    print('-' * 70)
    for error in statistics['errors'][:10]:
        print(f"  ‚Ä¢ {error['brand']} {error['model']}")
        print(f"    {error['url']}")
        print(f"    –û—à–∏–±–∫–∞: {error['error']}")
    if len(statistics['errors']) > 10:
        print(f"  ... –∏ –µ—â–µ {len(statistics['errors']) - 10} –æ—à–∏–±–æ–∫")

print(f"\n{'=' * 70}")
print("–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
print("  ‚Ä¢ drom_scraped_data_progress.json")
print("  ‚Ä¢ drom_scraped_data_progress.xlsx")
print('=' * 70)