"""
–£–õ–£–ß–®–ï–ù–ù–´–ô –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ü–ê–†–°–ï–† –° –ê–í–¢–û-–ü–û–í–¢–û–†–û–ú –û–®–ò–ë–û–ö
- –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∏–≤–∞–µ—Ç –¥–æ –∫–æ–Ω—Ü–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä—Å–∏–Ω–≥
- –ü–æ—Ç–æ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –≤—Å–µ –æ—à–∏–±–∫–∏
- –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ–ø—ã—Ç–∫–∏ –Ω–∞ –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É
"""

import pandas as pd
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import random
import re
import time
import os
import json
from typing import Dict, Any, Optional, List, Tuple, Set

# –ü—Ä–æ–≤–µ—Ä—è–µ–º brotli
try:
    import brotli
    print("‚úÖ Brotli —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
except ImportError:
    print("‚ùå –û–®–ò–ë–ö–ê: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ brotli –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
    exit(1)

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
]

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
CONCURRENT_REQUESTS = 7  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
SAVE_BATCH_SIZE = 50  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
CHUNK_SIZE = 50000  # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (50,000 –∑–∞–ø–∏—Å–µ–π)
START_INDEX = 36578  # –ù–∞—á–∏–Ω–∞–µ–º —Å —ç—Ç–æ–π –∑–∞–ø–∏—Å–∏
MAX_RETRY_ATTEMPTS = 3  # –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –æ—à–∏–±–∫–∏


def get_headers():
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }


def parse_specifications_table(soup: BeautifulSoup) -> Dict[str, str]:
    """–ü–∞—Ä—Å–∏—Ç —Ç–∞–±–ª–∏—Ü—É —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
    specs = {}
    table = soup.find('table', {'class': 'i2nf564', 'data-ftid': 'bulletin-specifications'})

    if not table:
        return specs

    rows = table.find_all('tr')
    for row in rows:
        property_cell = row.find('th', {'data-ftid': 'property'})
        value_cell = row.find('td', {'data-ftid': 'value'})

        if property_cell and value_cell:
            property_name = property_cell.get_text(strip=True)

            # –£–¥–∞–ª—è–µ–º –∫–Ω–æ–ø–∫—É "–Ω–∞–ª–æ–≥"
            button = value_cell.find('button')
            if button:
                button.decompose()

            # –ó–∞–º–µ–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–µ–∫—Å—Ç
            for link in value_cell.find_all('a'):
                link.replace_with(link.get_text(strip=True))

            value_text = value_cell.get_text(strip=True)
            value_text = re.sub(r'\s+', ' ', value_text)

            specs[property_name] = value_text

    return specs


def parse_vin_report(soup: BeautifulSoup) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç –±–ª–æ–∫ –æ—Ç—á–µ—Ç–∞ –ø–æ VIN"""
    vin_info = {
        'vin_full': None,
        'report_items': []
    }

    vin_block = soup.find('div', {'data-ga-stats-name': 'gibdd_report'})

    if not vin_block:
        return vin_info

    # VIN –Ω–æ–º–µ—Ä
    vin_div = vin_block.find('div', class_='css-o8yr01')
    if vin_div:
        vin_info['vin_full'] = vin_div.get_text(strip=True)

    # –ü—É–Ω–∫—Ç—ã –æ—Ç—á–µ—Ç–∞
    report_items_divs = vin_block.find_all('div', class_=re.compile(r'css-13qo6o5|css-z05wok'))

    for item_div in report_items_divs:
        button = item_div.find('button')
        if button:
            text = button.get_text(strip=True)
        else:
            text = item_div.get_text(strip=True)

        if text and len(text) > 3:
            vin_info['report_items'].append(text)

    return vin_info


def parse_description(soup: BeautifulSoup) -> Dict[str, str]:
    """–ü–∞—Ä—Å–∏—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
    description_data = {
        'full_description': '',
        'exchange_possible': '',
        'city_from_description': ''
    }

    desc_block = soup.find('div', {'data-ftid': 'bulletin-description'})

    if not desc_block:
        return description_data

    # –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
    full_desc_div = desc_block.find('div', {'data-ftid': 'info-full'})
    if full_desc_div:
        value_span = full_desc_div.find('span', {'data-ftid': 'value'})
        if value_span:
            for br in value_span.find_all('br'):
                br.replace_with('\n')

            description_text = value_span.get_text(strip=False)
            description_text = re.sub(r'\n\s*\n', '\n', description_text)
            description_data['full_description'] = description_text.strip()

    # –û–±–º–µ–Ω
    trade_div = desc_block.find('div', {'data-ftid': 'trade'})
    if trade_div:
        value_span = trade_div.find('span', {'data-ftid': 'value'})
        if value_span:
            description_data['exchange_possible'] = value_span.get_text(strip=True)

    # –ì–æ—Ä–æ–¥
    city_div = desc_block.find('div', {'data-ftid': 'city'})
    if city_div:
        value_span = city_div.find('span', {'data-ftid': 'value'})
        if value_span:
            description_data['city_from_description'] = value_span.get_text(strip=True)

    return description_data


def parse_bulletin_info(soup: BeautifulSoup) -> Dict[str, str]:
    """–ü–∞—Ä—Å–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
    bulletin_info = {
        'bulletin_id': '',
        'bulletin_date': '',
        'views_count': ''
    }

    info_block = soup.find('div', {'data-ftid': 'bull-page_bull-views'})

    if not info_block:
        return bulletin_info

    # –ù–æ–º–µ—Ä –∏ –¥–∞—Ç–∞
    bulletin_text_div = info_block.find('div', class_='css-pxeubi')
    if bulletin_text_div:
        text = bulletin_text_div.get_text(strip=True)
        match = re.search(r'–û–±—ä—è–≤–ª–µ–Ω–∏–µ\s+(\d+)\s+–æ—Ç\s+([\d.]+)', text)
        if match:
            bulletin_info['bulletin_id'] = match.group(1)
            bulletin_info['bulletin_date'] = match.group(2)

    # –ü—Ä–æ—Å–º–æ—Ç—Ä—ã
    views_div = info_block.find('div', class_='css-14wh0pm')
    if views_div:
        views_text = views_div.get_text(strip=True)
        views_match = re.search(r'(\d+)', views_text)
        if views_match:
            bulletin_info['views_count'] = views_match.group(1)

    return bulletin_info


async def scrape_listing_details(session: aiohttp.ClientSession, url: str, idx: int) -> Tuple[int, Optional[Dict[str, Any]]]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–∫—Ä–∞–ø–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
    try:
        async with session.get(url, headers=get_headers(), timeout=aiohttp.ClientTimeout(total=30)) as response:
            if response.status != 200:
                return (idx, None)

            # aiohttp –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç gzip/brotli
            html = await response.text(encoding='windows-1251')
            soup = BeautifulSoup(html, 'html.parser')

            specs = parse_specifications_table(soup)
            vin_report = parse_vin_report(soup)
            description = parse_description(soup)
            bulletin_info = parse_bulletin_info(soup)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤ –ø–ª–æ—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è Excel
            result = {
                # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                'engine': specs.get('–î–≤–∏–≥–∞—Ç–µ–ª—å', ''),
                'power': specs.get('–ú–æ—â–Ω–æ—Å—Ç—å', ''),
                'transmission': specs.get('–ö–æ—Ä–æ–±–∫–∞ –ø–µ—Ä–µ–¥–∞—á', ''),
                'drive': specs.get('–ü—Ä–∏–≤–æ–¥', ''),
                'body_type': specs.get('–¢–∏–ø –∫—É–∑–æ–≤–∞', ''),
                'color': specs.get('–¶–≤–µ—Ç', ''),
                'mileage_detail': specs.get('–ü—Ä–æ–±–µ–≥', ''),
                'owners': specs.get('–í–ª–∞–¥–µ–ª—å—Ü—ã', ''),
                'wheel': specs.get('–†—É–ª—å', ''),
                'generation': specs.get('–ü–æ–∫–æ–ª–µ–Ω–∏–µ', ''),
                'complectation': specs.get('–ö–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è', ''),

                # VIN –æ—Ç—á–µ—Ç
                'vin_full': vin_report.get('vin_full', ''),
                'vin_report_items': ' | '.join(vin_report.get('report_items', [])),

                # –û–ø–∏—Å–∞–Ω–∏–µ
                'full_description': description.get('full_description', ''),
                'exchange_possible': description.get('exchange_possible', ''),
                'city_from_description': description.get('city_from_description', ''),

                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏
                'bulletin_id': bulletin_info.get('bulletin_id', ''),
                'bulletin_date': bulletin_info.get('bulletin_date', ''),
                'views_count': bulletin_info.get('views_count', '')
            }

            return (idx, result)

    except Exception as e:
        return (idx, None)


def get_file_number(idx: int) -> int:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–æ–º–µ—Ä —Ñ–∞–π–ª–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É –∑–∞–ø–∏—Å–∏"""
    return (idx // CHUNK_SIZE) + 1


def get_file_path(file_number: int) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ø–æ –Ω–æ–º–µ—Ä—É"""
    return os.path.join(SCRIPT_DIR, f'drom_full_scraper_{file_number}.xlsx')


def load_or_create_chunk_file(file_number: int, source_df: pd.DataFrame) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame"""
    file_path = get_file_path(file_number)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
    start_idx = (file_number - 1) * CHUNK_SIZE
    end_idx = min(file_number * CHUNK_SIZE, len(source_df))

    if os.path.exists(file_path):
        try:
            return pd.read_excel(file_path)
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π")

    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª –∏–∑ –Ω—É–∂–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    chunk_df = source_df.iloc[start_idx:end_idx].copy()

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    new_columns = [
        'engine', 'power', 'transmission', 'drive', 'body_type', 'color',
        'mileage_detail', 'owners', 'wheel', 'generation', 'complectation',
        'vin_full', 'vin_report_items', 'full_description', 'exchange_possible',
        'city_from_description', 'bulletin_id', 'bulletin_date', 'views_count'
    ]

    for col in new_columns:
        if col not in chunk_df.columns:
            chunk_df[col] = ''

    return chunk_df


def load_progress():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å"""
    progress_file = os.path.join(SCRIPT_DIR, 'drom_full_scraper_progress.json')
    if os.path.exists(progress_file):
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return None


def save_progress(last_index, successful, failed, skipped, failed_indices: Set[int], retry_attempt: int = 1):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å"""
    progress_file = os.path.join(SCRIPT_DIR, 'drom_full_scraper_progress.json')
    with open(progress_file, 'w', encoding='utf-8') as f:
        json.dump({
            'last_index': last_index,
            'successful': successful,
            'failed': failed,
            'skipped': skipped,
            'failed_indices': list(failed_indices),
            'retry_attempt': retry_attempt
        }, f, ensure_ascii=False, indent=2)


async def process_batch(session: aiohttp.ClientSession, tasks: List[Tuple[int, str]]) -> List[Tuple[int, Optional[Dict]]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
    coroutines = [scrape_listing_details(session, url, idx) for idx, url in tasks]
    return await asyncio.gather(*coroutines)


async def main():
    start_time = time.time()

    print(f"\n{'=' * 80}")
    print("–£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ï–† –° –ê–í–¢–û-–ü–û–í–¢–û–†–û–ú –û–®–ò–ë–û–ö")
    print('=' * 80)
    print(f"‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {CONCURRENT_REQUESTS}")
    print(f"üì¶ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {CHUNK_SIZE:,} –∑–∞–ø–∏—Å–µ–π")
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ: {SAVE_BATCH_SIZE} —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
    print(f"üîÑ –ú–∞–∫—Å. –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –æ—à–∏–±–æ–∫: {MAX_RETRY_ATTEMPTS}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª
    input_file = os.path.join(SCRIPT_DIR, 'drom_scraped_data_progress.xlsx')

    if not os.path.exists(input_file):
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    print(f"\nüìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: drom_scraped_data_progress.xlsx")
    source_df = pd.read_excel(input_file)
    print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(source_df):,}")

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
    rows_to_process = source_df[source_df['status'] == '–ù–∞–π–¥–µ–Ω–æ']
    print(f"   –°–æ —Å—Ç–∞—Ç—É—Å–æ–º '–ù–∞–π–¥–µ–Ω–æ': {len(rows_to_process):,}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
    progress_data = load_progress()

    if progress_data:
        start_index = max(START_INDEX, progress_data.get('last_index', -1) + 1)
        successful = progress_data.get('successful', 0)
        failed = progress_data.get('failed', 0)
        skipped = progress_data.get('skipped', 0)
        failed_indices = set(progress_data.get('failed_indices', []))
        retry_attempt = progress_data.get('retry_attempt', 1)
        print(f"\nüîÑ –ü–†–û–î–û–õ–ñ–ê–ï–ú –° –°–¢–†–û–ö–ò {start_index:,}")
        print(f"   –û—à–∏–±–æ–∫ –≤ –±–∞–∑–µ: {len(failed_indices):,}")
        print(f"   –ü–æ–ø—ã—Ç–∫–∞: {retry_attempt}")
    else:
        start_index = START_INDEX
        successful = 0
        failed = 0
        skipped = 0
        failed_indices = set()
        retry_attempt = 1
        print(f"\nüÜï –ù–ê–ß–ò–ù–ê–ï–ú –°–û –°–¢–†–û–ö–ò {start_index:,}")

    print(f"{'=' * 80}\n")

    # –ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    current_chunk_number = None
    current_chunk_df = None

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è
    connector = aiohttp.TCPConnector(limit=CONCURRENT_REQUESTS, limit_per_host=CONCURRENT_REQUESTS)
    timeout = aiohttp.ClientTimeout(total=30)

    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        try:
            # ===== –≠–¢–ê–ü 1: –û–°–ù–û–í–ù–û–ô –ü–ê–†–°–ò–ù–ì –î–û –ö–û–ù–¶–ê =====
            print(f"{'='*80}")
            print("–≠–¢–ê–ü 1: –û–°–ù–û–í–ù–û–ô –ü–ê–†–°–ò–ù–ì –î–û –ö–û–ù–¶–ê")
            print(f"{'='*80}\n")

            batch_tasks = []
            idx = start_index

            while idx < len(source_df):
                row = source_df.iloc[idx]
                status = row.get('status', '')
                url = row.get('url', '')

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ "–ù–∞–π–¥–µ–Ω–æ"
                if status != '–ù–∞–π–¥–µ–Ω–æ':
                    skipped += 1
                    idx += 1
                    continue

                if not url:
                    skipped += 1
                    idx += 1
                    continue

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞—Ç—á
                batch_tasks.append((idx, url))

                # –ö–æ–≥–¥–∞ –±–∞—Ç—á –∑–∞–ø–æ–ª–Ω–µ–Ω - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                if len(batch_tasks) >= CONCURRENT_REQUESTS:
                    results = await process_batch(session, batch_tasks)

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    for result_idx, details in results:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–π–ª –¥–ª—è —ç—Ç–æ–π –∑–∞–ø–∏—Å–∏
                        file_number = get_file_number(result_idx)

                        # –ó–∞–≥—Ä—É–∂–∞–µ–º chunk –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                        if file_number != current_chunk_number:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π chunk
                            if current_chunk_df is not None and current_chunk_number is not None:
                                file_path = get_file_path(current_chunk_number)
                                current_chunk_df.to_excel(file_path, index=False)

                            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–π chunk
                            current_chunk_number = file_number
                            current_chunk_df = load_or_create_chunk_file(file_number, source_df)

                        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        chunk_idx = result_idx % CHUNK_SIZE

                        if details:
                            for key, value in details.items():
                                current_chunk_df.at[chunk_idx, key] = value
                            successful += 1

                            # –£–±–∏—Ä–∞–µ–º –∏–∑ —Å–ø–∏—Å–∫–∞ –æ—à–∏–±–æ–∫ –µ—Å–ª–∏ –±—ã–ª–∞ —Ç–∞–º
                            if result_idx in failed_indices:
                                failed_indices.remove(result_idx)
                                failed -= 1

                            # –£–ª—É—á—à–µ–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                            total_processed = successful + failed
                            progress_pct = (result_idx / len(source_df)) * 100
                            success_rate = (successful / total_processed * 100) if total_processed > 0 else 0

                            car_name = source_df.iloc[result_idx].get('car_name', 'N/A')
                            vin = details.get('vin_full', 'N/A')[:8] if details.get('vin_full') else 'N/A'
                            views = details.get('views_count', 'N/A')

                            print(f"[{result_idx + 1:,}/{len(source_df):,}] ({progress_pct:.1f}%) ‚úì {car_name} | VIN: {vin}... | –ü—Ä–æ—Å–º–æ—Ç—Ä—ã: {views} | –£—Å–ø–µ—à–Ω–æ: {successful:,} | –û—à–∏–±–æ–∫: {failed:,} | Success Rate: {success_rate:.1f}%")
                        else:
                            failed += 1
                            failed_indices.add(result_idx)

                            total_processed = successful + failed
                            progress_pct = (result_idx / len(source_df)) * 100
                            success_rate = (successful / total_processed * 100) if total_processed > 0 else 0

                            car_name = source_df.iloc[result_idx].get('car_name', 'N/A')
                            print(f"[{result_idx + 1:,}/{len(source_df):,}] ({progress_pct:.1f}%) ‚úó {car_name} | –£—Å–ø–µ—à–Ω–æ: {successful:,} | –û—à–∏–±–æ–∫: {failed:,} | Success Rate: {success_rate:.1f}%")

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                        if successful % SAVE_BATCH_SIZE == 0:
                            save_progress(result_idx, successful, failed, skipped, failed_indices, retry_attempt)
                            if current_chunk_df is not None and current_chunk_number is not None:
                                file_path = get_file_path(current_chunk_number)
                                current_chunk_df.to_excel(file_path, index=False)

                                # –†–∞—Å—á–µ—Ç ETA
                                elapsed = time.time() - start_time
                                items_per_sec = successful / elapsed if elapsed > 0 else 0
                                remaining_items = len(rows_to_process) - successful
                                eta_seconds = remaining_items / items_per_sec if items_per_sec > 0 else 0
                                eta_hours = eta_seconds / 3600

                                print(f"    üíæ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω | –£—Å–ø–µ—à–Ω–æ: {successful:,} | –û—à–∏–±–æ–∫: {failed:,} | –°–∫–æ—Ä–æ—Å—Ç—å: {items_per_sec:.1f} items/sec | ETA: {eta_hours:.1f}—á")

                    batch_tasks = []

                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                    await asyncio.sleep(random.uniform(0.3, 0.7))

                idx += 1

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–¥–∞—á–∏
            if batch_tasks:
                results = await process_batch(session, batch_tasks)

                for result_idx, details in results:
                    file_number = get_file_number(result_idx)

                    if file_number != current_chunk_number:
                        if current_chunk_df is not None and current_chunk_number is not None:
                            file_path = get_file_path(current_chunk_number)
                            current_chunk_df.to_excel(file_path, index=False)

                        current_chunk_number = file_number
                        current_chunk_df = load_or_create_chunk_file(file_number, source_df)

                    chunk_idx = result_idx % CHUNK_SIZE

                    if details:
                        for key, value in details.items():
                            current_chunk_df.at[chunk_idx, key] = value
                        successful += 1

                        if result_idx in failed_indices:
                            failed_indices.remove(result_idx)
                            failed -= 1

                        total_processed = successful + failed
                        progress_pct = (result_idx / len(source_df)) * 100
                        success_rate = (successful / total_processed * 100) if total_processed > 0 else 0

                        car_name = source_df.iloc[result_idx].get('car_name', 'N/A')
                        vin = details.get('vin_full', 'N/A')[:8] if details.get('vin_full') else 'N/A'
                        views = details.get('views_count', 'N/A')

                        print(f"[{result_idx + 1:,}/{len(source_df):,}] ({progress_pct:.1f}%) ‚úì {car_name} | VIN: {vin}... | –ü—Ä–æ—Å–º–æ—Ç—Ä—ã: {views} | –£—Å–ø–µ—à–Ω–æ: {successful:,} | –û—à–∏–±–æ–∫: {failed:,} | Success Rate: {success_rate:.1f}%")
                    else:
                        failed += 1
                        failed_indices.add(result_idx)

                        total_processed = successful + failed
                        progress_pct = (result_idx / len(source_df)) * 100
                        success_rate = (successful / total_processed * 100) if total_processed > 0 else 0

                        car_name = source_df.iloc[result_idx].get('car_name', 'N/A')
                        print(f"[{result_idx + 1:,}/{len(source_df):,}] ({progress_pct:.1f}%) ‚úó {car_name} | –£—Å–ø–µ—à–Ω–æ: {successful:,} | –û—à–∏–±–æ–∫: {failed:,} | Success Rate: {success_rate:.1f}%")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
            if current_chunk_df is not None and current_chunk_number is not None:
                file_path = get_file_path(current_chunk_number)
                current_chunk_df.to_excel(file_path, index=False)

            save_progress(len(source_df) - 1, successful, failed, skipped, failed_indices, retry_attempt)

            print(f"\n{'='*80}")
            print("–≠–¢–ê–ü 1 –ó–ê–í–ï–†–®–ï–ù!")
            print(f"{'='*80}")
            print(f"–£—Å–ø–µ—à–Ω–æ: {successful:,} | –û—à–∏–±–æ–∫: {len(failed_indices):,}")

            # ===== –≠–¢–ê–ü 2: –ü–û–í–¢–û–†–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –û–®–ò–ë–û–ö =====
            while len(failed_indices) > 0 and retry_attempt < MAX_RETRY_ATTEMPTS:
                retry_attempt += 1

                print(f"\n{'='*80}")
                print(f"–≠–¢–ê–ü 2: –ü–û–í–¢–û–†–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –û–®–ò–ë–û–ö (–ü–æ–ø—ã—Ç–∫–∞ {retry_attempt}/{MAX_RETRY_ATTEMPTS})")
                print(f"{'='*80}")
                print(f"–û—à–∏–±–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(failed_indices):,}")
                print(f"{'='*80}\n")

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏
                failed_list = sorted(list(failed_indices))
                batch_tasks = []
                processed_count = 0

                for idx in failed_list:
                    row = source_df.iloc[idx]
                    url = row.get('url', '')

                    if not url:
                        continue

                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞—Ç—á
                    batch_tasks.append((idx, url))

                    # –ö–æ–≥–¥–∞ –±–∞—Ç—á –∑–∞–ø–æ–ª–Ω–µ–Ω - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                    if len(batch_tasks) >= CONCURRENT_REQUESTS:
                        results = await process_batch(session, batch_tasks)

                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        for result_idx, details in results:
                            processed_count += 1

                            file_number = get_file_number(result_idx)

                            if file_number != current_chunk_number:
                                if current_chunk_df is not None and current_chunk_number is not None:
                                    file_path = get_file_path(current_chunk_number)
                                    current_chunk_df.to_excel(file_path, index=False)

                                current_chunk_number = file_number
                                current_chunk_df = load_or_create_chunk_file(file_number, source_df)

                            chunk_idx = result_idx % CHUNK_SIZE

                            if details:
                                for key, value in details.items():
                                    current_chunk_df.at[chunk_idx, key] = value
                                successful += 1
                                failed -= 1
                                failed_indices.remove(result_idx)

                                car_name = source_df.iloc[result_idx].get('car_name', 'N/A')
                                vin = details.get('vin_full', 'N/A')[:8] if details.get('vin_full') else 'N/A'
                                views = details.get('views_count', 'N/A')

                                print(f"[Retry {processed_count:,}/{len(failed_list):,}] ‚úì {car_name} | VIN: {vin}... | –û—Å—Ç–∞–ª–æ—Å—å –æ—à–∏–±–æ–∫: {len(failed_indices):,}")
                            else:
                                car_name = source_df.iloc[result_idx].get('car_name', 'N/A')
                                print(f"[Retry {processed_count:,}/{len(failed_list):,}] ‚úó {car_name} | –û—Å—Ç–∞–ª–æ—Å—å –æ—à–∏–±–æ–∫: {len(failed_indices):,}")

                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                            if processed_count % SAVE_BATCH_SIZE == 0:
                                save_progress(result_idx, successful, failed, skipped, failed_indices, retry_attempt)
                                if current_chunk_df is not None and current_chunk_number is not None:
                                    file_path = get_file_path(current_chunk_number)
                                    current_chunk_df.to_excel(file_path, index=False)
                                    print(f"    üíæ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω | –£—Å–ø–µ—à–Ω–æ: {successful:,} | –û—à–∏–±–æ–∫: {len(failed_indices):,}")

                        batch_tasks = []
                        await asyncio.sleep(random.uniform(0.5, 1.0))  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –ø–∞—É–∑–∞ –ø—Ä–∏ —Ä–µ—Ç—Ä–∞—è—Ö

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è
                if batch_tasks:
                    results = await process_batch(session, batch_tasks)

                    for result_idx, details in results:
                        processed_count += 1

                        file_number = get_file_number(result_idx)

                        if file_number != current_chunk_number:
                            if current_chunk_df is not None and current_chunk_number is not None:
                                file_path = get_file_path(current_chunk_number)
                                current_chunk_df.to_excel(file_path, index=False)

                            current_chunk_number = file_number
                            current_chunk_df = load_or_create_chunk_file(file_number, source_df)

                        chunk_idx = result_idx % CHUNK_SIZE

                        if details:
                            for key, value in details.items():
                                current_chunk_df.at[chunk_idx, key] = value
                            successful += 1
                            failed -= 1
                            failed_indices.remove(result_idx)

                            car_name = source_df.iloc[result_idx].get('car_name', 'N/A')
                            vin = details.get('vin_full', 'N/A')[:8] if details.get('vin_full') else 'N/A'
                            views = details.get('views_count', 'N/A')

                            print(f"[Retry {processed_count:,}/{len(failed_list):,}] ‚úì {car_name} | VIN: {vin}... | –û—Å—Ç–∞–ª–æ—Å—å –æ—à–∏–±–æ–∫: {len(failed_indices):,}")
                        else:
                            car_name = source_df.iloc[result_idx].get('car_name', 'N/A')
                            print(f"[Retry {processed_count:,}/{len(failed_list):,}] ‚úó {car_name} | –û—Å—Ç–∞–ª–æ—Å—å –æ—à–∏–±–æ–∫: {len(failed_indices):,}")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –ø–æ –æ—à–∏–±–∫–∞–º
                if current_chunk_df is not None and current_chunk_number is not None:
                    file_path = get_file_path(current_chunk_number)
                    current_chunk_df.to_excel(file_path, index=False)

                save_progress(len(source_df) - 1, successful, failed, skipped, failed_indices, retry_attempt)

                print(f"\n{'='*80}")
                print(f"–ü–û–ü–´–¢–ö–ê {retry_attempt} –ó–ê–í–ï–†–®–ï–ù–ê")
                print(f"{'='*80}")
                print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤ —ç—Ç–æ–π –ø–æ–ø—ã—Ç–∫–µ: {len(failed_list) - len(failed_indices):,}")
                print(f"–û—Å—Ç–∞–ª–æ—Å—å –æ—à–∏–±–æ–∫: {len(failed_indices):,}")

        except KeyboardInterrupt:
            print("\n\n‚ö† –ü–†–ï–†–í–ê–ù–û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ú")

        finally:
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            if current_chunk_df is not None and current_chunk_number is not None:
                file_path = get_file_path(current_chunk_number)
                current_chunk_df.to_excel(file_path, index=False)

            save_progress(len(source_df) - 1, successful, failed, skipped, failed_indices, retry_attempt)

            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            elapsed_time = time.time() - start_time
            elapsed_hours = elapsed_time / 3600
            items_per_hour = successful / elapsed_hours if elapsed_hours > 0 else 0

            print(f"\n{'=' * 80}")
            print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
            print('=' * 80)
            print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫:           {len(source_df):,}")
            print(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ:  {successful:,}")
            print(f"‚úó –§–∏–Ω–∞–ª—å–Ω—ã—Ö –æ—à–∏–±–æ–∫:    {len(failed_indices):,}")
            print(f"‚óã –ü—Ä–æ–ø—É—â–µ–Ω–æ:           {skipped:,}")
            print(f"\n‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:    {elapsed_hours:.2f} —á–∞—Å–æ–≤")
            print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å:             {items_per_hour:.0f} items/—á–∞—Å")
            print(f"üîÑ –ü–æ–ø—ã—Ç–æ–∫ —Å–¥–µ–ª–∞–Ω–æ:     {retry_attempt}")
            print(f"\n–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {SCRIPT_DIR}")
            print('=' * 80)


if __name__ == '__main__':
    asyncio.run(main())
